{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNcjAn4hSScW3kRqxo3wYJH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# MediaPipe-Based Eye Region Extraction\n","\n","This notebook preprocesses raw videos using MediaPipe Face Mesh to:\n","- Detect eye regions frame-by-frame\n","- Extract eye bounding boxes and landmarks\n","- Save metadata for downstream ViViT training\n","\n","**Output**\n","- Eye metadata (`.npz`): bounding boxes + landmarks per frame\n","- Full-frame BMPs aligned with metadata\n","\n","This preprocessing is used by the ViViT blink classification model.\n"],"metadata":{"id":"BdooMLH-vJ0W"}},{"cell_type":"markdown","source":["## 1. Imports and Environment Setup\n"],"metadata":{"id":"Y4TIwW2XvcE7"}},{"cell_type":"code","source":["import os\n","import cv2\n","import numpy as np\n","import mediapipe as mp\n","from tqdm import tqdm"],"metadata":{"id":"xksR7wD5w19p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. MediaPipe Face Mesh Initialization\n"],"metadata":{"id":"ykyw9n3nvriW"}},{"cell_type":"code","source":["mp_face_mesh = mp.solutions.face_mesh\n","\n","face_mesh = mp_face_mesh.FaceMesh(\n","    static_image_mode=False,      # video stream\n","    max_num_faces=1,              # single subject\n","    refine_landmarks=True,        # enables iris landmarks\n","    min_detection_confidence=0.5,\n","    min_tracking_confidence=0.5,\n",")\n"],"metadata":{"id":"phE1MENTvx0t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Eye Landmark Definitions\n"],"metadata":{"id":"UCtJVkpwvwRv"}},{"cell_type":"code","source":["# MediaPipe FaceMesh eye landmark indices\n","LEFT_EYE_IDX = [\n","    33, 133, 160, 159, 158, 157, 173,\n","    246, 161, 163, 144, 145, 153, 154, 155\n","]\n","\n","RIGHT_EYE_IDX = [\n","    362, 263, 387, 386, 385, 384, 398,\n","    466, 388, 390, 373, 374, 380, 381, 382\n","]\n","\n","EYE_LANDMARKS = LEFT_EYE_IDX + RIGHT_EYE_IDX\n"],"metadata":{"id":"DXTvHur-vv_x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Frame-by-Frame Eye Detection\n"],"metadata":{"id":"6JKJp1FOv6CN"}},{"cell_type":"code","source":["def eye_bbox_from_landmarks(landmarks, frame_shape, margin=0.15):\n","    \"\"\"\n","    landmarks: list of (x,y) normalized coords\n","    frame_shape: (H,W,3)\n","    \"\"\"\n","    h, w = frame_shape[:2]\n","    xs = [int(lm.x * w) for lm in landmarks]\n","    ys = [int(lm.y * h) for lm in landmarks]\n","\n","    x1, x2 = min(xs), max(xs)\n","    y1, y2 = min(ys), max(ys)\n","\n","    # expand bbox slightly\n","    bw, bh = x2 - x1, y2 - y1\n","    pad_w, pad_h = int(bw * margin), int(bh * margin)\n","\n","    x1 = max(0, x1 - pad_w)\n","    y1 = max(0, y1 - pad_h)\n","    x2 = min(w, x2 + pad_w)\n","    y2 = min(h, y2 + pad_h)\n","\n","    return x1, y1, x2 - x1, y2 - y1\n"],"metadata":{"id":"PktoT4nPv8f_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6. Metadata Serialization\n"],"metadata":{"id":"YuCcoiEfwG8a"}},{"cell_type":"code","source":["def process_video_extract_eyes(\n","    video_path,\n","    save_root,\n","    video_id,\n","):\n","    cap = cv2.VideoCapture(video_path)\n","    bboxes = []\n","    landmarks_all = []\n","\n","    os.makedirs(save_root, exist_ok=True)\n","\n","    last_bbox, last_landmarks = None, None\n","    frame_idx = 0\n","\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","        result = face_mesh.process(rgb)\n","\n","        if result.multi_face_landmarks:\n","            face = result.multi_face_landmarks[0]\n","            eye_lms = [face.landmark[i] for i in EYE_LANDMARKS]\n","\n","            bbox = eye_bbox_from_landmarks(eye_lms, frame.shape)\n","            bboxes.append(bbox)\n","            landmarks_all.append([(lm.x, lm.y) for lm in eye_lms])\n","\n","            last_bbox, last_landmarks = bbox, eye_lms\n","        else:\n","            # fallback to last valid detection\n","            if last_bbox is not None:\n","                bboxes.append(last_bbox)\n","                landmarks_all.append([(lm.x, lm.y) for lm in last_landmarks])\n","\n","        frame_idx += 1\n","\n","    cap.release()\n","\n","    np.savez_compressed(\n","        os.path.join(save_root, f\"{video_id}_metadata.npz\"),\n","        bboxes=np.array(bboxes),\n","        landmarks=np.array(landmarks_all),\n","    )\n"],"metadata":{"id":"qPGozsP5wGA-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7. Directory Structure and Output Format\n"],"metadata":{"id":"epZMGtXgwNr1"}},{"cell_type":"markdown","source":["training/\n","└── blink/\n","└── <video_id>/\n","└── 0/\n","├── 0.bmp\n","├── 1.bmp\n","\n","\n","eye_dataset_train/\n","└── blink/\n","└── <video_id>_metadata.npz"],"metadata":{"id":"bNBZEA_Dy_Gw"}},{"cell_type":"markdown","source":["### Why metadata instead of raw crops?\n","Saving eye metadata allows:\n","- Reproducible cropping\n","- Different crop strategies without rerunning MediaPipe\n","- Efficient experimentation\n"],"metadata":{"id":"LSmCSI2JwaC-"}}]}