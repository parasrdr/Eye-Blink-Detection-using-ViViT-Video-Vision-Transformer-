{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ViViT Inference and Attention Visualization for Eye Blink Detection\n",
        "\n",
        "This notebook performs inference on unseen videos and visualizes\n",
        "spatiotemporal attention maps produced by ViViT.\n"
      ],
      "metadata": {
        "id": "mEFhJBSf2oTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Imports and Model Loading\n",
        "\n",
        "We load the trained ViViT model with attention outputs enabled\n",
        "to analyze temporal and spatial focus during blink detection.\n"
      ],
      "metadata": {
        "id": "Ic3i8LgM2yUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import VivitForVideoClassification\n"
      ],
      "metadata": {
        "id": "MXM6gXqCFWJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = VivitForVideoClassification.from_pretrained(\n",
        "    \"google/vivit-b-16x2-kinetics400\",\n",
        "    num_labels=2,\n",
        "    ignore_mismatched_sizes=True,\n",
        "    output_attentions=True,\n",
        ").to(device)\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/best_model(1).pth\"\n",
        "state = torch.load(checkpoint_path, map_location=\"cpu\")\n",
        "\n",
        "if \"state_dict\" in state:\n",
        "    state = state[\"state_dict\"]\n",
        "\n",
        "model.load_state_dict(state, strict=False)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVuwc7uUFo9Z",
        "outputId": "f3dace3e-7382-4d83-9918-842476599091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of VivitForVideoClassification were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VivitForVideoClassification(\n",
              "  (vivit): VivitModel(\n",
              "    (embeddings): VivitEmbeddings(\n",
              "      (patch_embeddings): VivitTubeletEmbeddings(\n",
              "        (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): VivitEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x VivitLayer(\n",
              "          (attention): VivitAttention(\n",
              "            (attention): VivitSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (output): VivitSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): VivitIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (intermediate_act_fn): FastGELUActivation()\n",
              "          )\n",
              "          (output): VivitOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Video Loading and Preprocessing\n",
        "\n",
        "Input videos are resized, padded, and temporally aligned to match\n",
        "ViViT’s expected input format.\n"
      ],
      "metadata": {
        "id": "h_01hiDb3EFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_eye_video(\n",
        "    video_path,\n",
        "    target_frames=32,\n",
        "    output_hw=(96, 192),\n",
        "):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame = cv2.resize(frame, (output_hw[1], output_hw[0]))\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames) == 0:\n",
        "        raise ValueError(\"No frames loaded\")\n",
        "\n",
        "    real_frames = len(frames)\n",
        "    frames = np.array(frames, dtype=np.uint8)\n",
        "\n",
        "    if real_frames > target_frames:\n",
        "        idx = np.linspace(0, real_frames - 1, target_frames).astype(int)\n",
        "        frames = frames[idx]\n",
        "        real_frames = target_frames\n",
        "    elif real_frames < target_frames:\n",
        "        pad = target_frames - real_frames\n",
        "        frames = np.concatenate(\n",
        "            [frames, np.repeat(frames[-1:], pad, axis=0)], axis=0\n",
        "        )\n",
        "\n",
        "    return frames, real_frames\n"
      ],
      "metadata": {
        "id": "vEGLrafiKzRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_to_224(frames):\n",
        "    H, W = frames.shape[1], frames.shape[2]\n",
        "\n",
        "    pad_top = (224 - H) // 2\n",
        "    pad_bottom = 224 - H - pad_top\n",
        "    pad_left = (224 - W) // 2\n",
        "    pad_right = 224 - W - pad_left\n",
        "\n",
        "    return np.pad(\n",
        "        frames,\n",
        "        ((0, 0), (pad_top, pad_bottom), (pad_left, pad_right), (0, 0)),\n",
        "        mode=\"constant\",\n",
        "        constant_values=0,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "induK-pNFjEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/content/drive/MyDrive/eye_dataset_test/blink/164.mp4\"\n",
        "\n",
        "frames_orig, real_frames = load_eye_video(video_path)\n",
        "frames_224 = pad_to_224(frames_orig)\n",
        "\n",
        "video_tensor = torch.from_numpy(frames_224).float() / 255.0\n",
        "video_tensor = video_tensor.permute(0, 3, 1, 2)  # (T, 3, H, W)\n",
        "video_tensor = video_tensor.unsqueeze(0).to(device)  # (1, 3, T, H, W)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(video_tensor)\n"
      ],
      "metadata": {
        "id": "DHBbfgdWK-Y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Attention Rollout\n",
        "\n",
        "We extract attention maps from the final transformer layer and compute:\n",
        "- Temporal attention per tubelet\n",
        "- Spatial attention per frame\n"
      ],
      "metadata": {
        "id": "Dudi9BW93fuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_rollout_vivit(outputs):\n",
        "    attn = outputs.attentions[0][0].mean(0)\n",
        "    attn = attn[1:, 1:]\n",
        "\n",
        "    num_patches = 14 * 14\n",
        "    T_tokens = attn.shape[0] // num_patches\n",
        "\n",
        "    temporal_attn = []\n",
        "    spatial_attn = []\n",
        "\n",
        "    for t in range(T_tokens):\n",
        "        start = t * num_patches\n",
        "        end = (t + 1) * num_patches\n",
        "        block = attn[start:end, start:end]\n",
        "\n",
        "        temporal_attn.append(block.mean().item())\n",
        "        spatial_attn.append(block.mean(0).reshape(14, 14).cpu())\n",
        "\n",
        "    return np.array(temporal_attn), np.stack(spatial_attn)\n"
      ],
      "metadata": {
        "id": "BkhT25HyF6rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temporal_attn, spatial_attn = attention_rollout_vivit(outputs)\n",
        "\n",
        "tubelet_size = 2\n",
        "real_tokens = int(np.ceil(real_frames / tubelet_size))\n",
        "\n",
        "temporal_attn = temporal_attn[:real_tokens]\n",
        "spatial_attn = spatial_attn[:real_tokens]\n"
      ],
      "metadata": {
        "id": "CTXr-0bBLE4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_spatial_attention(attn_14, orig_hw=(96, 192), padded_hw=(224, 224)):\n",
        "    H, W = padded_hw\n",
        "    h0 = (H - orig_hw[0]) // 2\n",
        "    w0 = (W - orig_hw[1]) // 2\n",
        "\n",
        "    attn_224 = cv2.resize(attn_14, (W, H))\n",
        "    return attn_224[h0:h0+orig_hw[0], w0:w0+orig_hw[1]]\n"
      ],
      "metadata": {
        "id": "mtBzEQTjGD0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Temporal Saliency Estimation\n",
        "\n",
        "Temporal saliency is computed by aggregating spatial attention across frames.\n",
        "This provides a soft indication of blink likelihood over time.\n"
      ],
      "metadata": {
        "id": "zuV22S3f3pCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_temporal_saliency(spatial_attn):\n",
        "    temporal = spatial_attn.mean(axis=(1, 2)).astype(np.float32)\n",
        "    temporal -= temporal.min()\n",
        "    temporal /= (temporal.max() + 1e-6)\n",
        "    return temporal\n"
      ],
      "metadata": {
        "id": "pmgH1wDYG2kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temporal_saliency = compute_temporal_saliency(spatial_attn)\n"
      ],
      "metadata": {
        "id": "xH8Lt_p_G5QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_blinks(temporal_saliency, smoothing=3, threshold_factor=0.5):\n",
        "    kernel = np.ones(smoothing) / smoothing\n",
        "    sal = np.convolve(temporal_saliency, kernel, mode=\"same\")\n",
        "\n",
        "    threshold = threshold_factor * sal.max()\n",
        "\n",
        "    blink_tokens = [\n",
        "        t for t in range(1, len(sal) - 1)\n",
        "        if sal[t] > threshold and sal[t] > sal[t-1] and sal[t] > sal[t+1]\n",
        "    ]\n",
        "\n",
        "    return blink_tokens, sal, threshold\n"
      ],
      "metadata": {
        "id": "Hu2qrCxaG_2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blink_tokens, temporal_smooth, blink_threshold = detect_blinks(temporal_saliency)\n",
        "blink_frames = [t * 2 for t in blink_tokens]\n"
      ],
      "metadata": {
        "id": "IgxzJloHHD2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Attention Overlay on Eye Frames\n",
        "\n",
        "Spatial attention maps are overlaid onto eye-region frames and modulated\n",
        "by temporal saliency to highlight blink-relevant frames.\n"
      ],
      "metadata": {
        "id": "m08f3La131Vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def overlay_attention_single_frame(\n",
        "    frame,\n",
        "    spatial_map,\n",
        "    temporal_saliency_t,\n",
        "    alpha=0.35,\n",
        "    gamma=2.5,\n",
        "    temporal_gain=3.0,\n",
        "):\n",
        "    attn = spatial_map.astype(np.float32)\n",
        "    attn /= (attn.max() + 1e-6)\n",
        "\n",
        "    attn *= np.exp(temporal_gain * temporal_saliency_t)\n",
        "    attn = attn ** gamma\n",
        "\n",
        "    attn -= attn.min()\n",
        "    attn /= (attn.max() + 1e-6)\n",
        "\n",
        "    heat = cv2.applyColorMap(\n",
        "        (attn * 255).astype(np.uint8),\n",
        "        cv2.COLORMAP_INFERNO\n",
        "    )\n",
        "\n",
        "    return cv2.addWeighted(frame, 1 - alpha, heat, alpha, 0)\n"
      ],
      "metadata": {
        "id": "2dV5wum6HMLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def overlay_blink_marker(\n",
        "    frame,\n",
        "    is_blink,\n",
        "    frame_idx,\n",
        "):\n",
        "    frame = frame.copy()\n",
        "    h, w = frame.shape[:2]\n",
        "\n",
        "    # ── tiny frame index (bottom-left)\n",
        "    cv2.putText(\n",
        "        frame,\n",
        "        f\"{frame_idx}\",\n",
        "        (4, h - 4),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        0.28,               # VERY small\n",
        "        (255, 255, 255),\n",
        "        1,\n",
        "        cv2.LINE_AA,\n",
        "    )\n",
        "\n",
        "    # ── subtle blink dot (top-right)\n",
        "    if is_blink:\n",
        "        cv2.circle(\n",
        "            frame,\n",
        "            (w - 12, 12),\n",
        "            4,               # small dot\n",
        "            (0, 255, 255),   # vibrant yellow\n",
        "            -1,\n",
        "        )\n",
        "\n",
        "    return frame\n"
      ],
      "metadata": {
        "id": "vwNcSeosMA1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Visualization Video Export\n",
        "\n",
        "We save:\n",
        "- Attention overlays\n",
        "- Side-by-side comparisons of original and attention-enhanced videos\n"
      ],
      "metadata": {
        "id": "tZlv_ss64DGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vis_frames = []\n",
        "\n",
        "for i, frame in enumerate(frames_orig[:real_frames]):\n",
        "    t = min(i // tubelet_size, real_tokens - 1)\n",
        "\n",
        "    spatial_map = crop_spatial_attention(\n",
        "        spatial_attn[t],\n",
        "        orig_hw=(96, 192),\n",
        "        padded_hw=(224, 224),\n",
        "    )\n",
        "\n",
        "    vis = overlay_attention_single_frame(\n",
        "        frame,\n",
        "        spatial_map,\n",
        "        temporal_saliency[t],\n",
        "    )\n",
        "\n",
        "    vis = overlay_blink_marker(vis, t in blink_tokens, i)\n",
        "    vis_frames.append(vis)\n"
      ],
      "metadata": {
        "id": "ENdFJAK_LU-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_video(frames, path, fps=12):\n",
        "    h, w, _ = frames[0].shape\n",
        "    writer = cv2.VideoWriter(\n",
        "        path,\n",
        "        cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
        "        fps,\n",
        "        (w, h)\n",
        "    )\n",
        "    for f in frames:\n",
        "        writer.write(f.astype(np.uint8))\n",
        "    writer.release()\n"
      ],
      "metadata": {
        "id": "dcVs-NvzHYb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_video(vis_frames, \"blink_attention_overlay.mp4\")\n"
      ],
      "metadata": {
        "id": "aLee5UjUHcom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stack_and_save_video(orig, overlay, path, fps=3):\n",
        "    stacked = [cv2.hconcat([o, v]) for o, v in zip(orig, overlay)]\n",
        "    save_video(stacked, path, fps)\n"
      ],
      "metadata": {
        "id": "o8vv7UawHj5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stack_and_save_video(\n",
        "    frames_orig,\n",
        "    vis_frames,\n",
        "    \"blink_attention_side_by_side.mp4\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "hLMVnKORHmuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output Artifacts\n",
        "\n",
        "- Attention overlay videos (`.mp4`)\n",
        "- Side-by-side visual comparisons\n",
        "- Temporal saliency curves (optional)\n"
      ],
      "metadata": {
        "id": "Gg5mZ5yt4N-u"
      }
    }
  ]
}